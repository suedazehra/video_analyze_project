#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""Video Analiz Sistemi """

import re
import os
import json
import requests
import subprocess
import whisper
from collections import Counter, defaultdict
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np
from pathlib import Path
import time

class VideoProcessor:
    """Video'dan ses Ã§Ä±karma ve transkript oluÅŸturma"""
    
    def __init__(self):
        self.temp_dir = "temp_audio"
        os.makedirs(self.temp_dir, exist_ok=True)
    
    def extract_audio_from_video(self, video_path, audio_path=None):
        """Video'dan ses Ã§Ä±kar"""
        if audio_path is None:
            video_name = Path(video_path).stem
            audio_path = f"{self.temp_dir}/{video_name}.wav"
        
        try:
            cmd = [
                'ffmpeg', '-i', video_path,
                '-vn',  # video stream'i dahil etme
                '-c:a', 'pcm_s16le',  # wav formatÄ±,  # wav formatÄ±
                '-ar', '16000',  # 16kHz sampling rate
                '-ac', '1',  # mono kanal
                '-y',  # Ã¼zerine yaz
                audio_path
            ]
            
            print(f"ğŸµ Video'dan ses Ã§Ä±karÄ±lÄ±yor: {video_path}")
            result = subprocess.run(cmd, capture_output=True, text=True, check=True)
            print(f"âœ… Ses dosyasÄ± oluÅŸturuldu: {audio_path}")
            return audio_path
            
        except subprocess.CalledProcessError as e:
            print(f"âŒ FFmpeg hatasÄ±: {e}")
            print(f"STDOUT: {e.stdout}")
            print(f"STDERR: {e.stderr}")
            return None
        except FileNotFoundError:
            print("âŒ FFmpeg bulunamadÄ±! LÃ¼tfen FFmpeg'i yÃ¼kleyin:")
            return None
    
    def check_dependencies(self):
        """Gerekli baÄŸÄ±mlÄ±lÄ±klarÄ± kontrol et"""
        dependencies = {
            'ffmpeg': self.check_ffmpeg(),
            'whisper': self.check_whisper()
        }
        return dependencies
    
    def check_ffmpeg(self):
        """FFmpeg kurulu mu kontrol et"""
        try:
            subprocess.run(['ffmpeg', '-version'], 
                         capture_output=True, check=True)
            return True
        except:
            return False
    
    def check_whisper(self):
        """Whisper kurulu mu kontrol et"""
        try:
            import whisper
            return True
        except ImportError:
            return False

class WhisperTranscriber:
    """Whisper ile ses tanÄ±ma"""
    
    def __init__(self, model_size="base"):
        self.model_size = model_size
        self.model = None
        self.load_model()
    
    def load_model(self):
        """Whisper modelini yÃ¼kle"""
        try:
            print(f"ğŸ¤– Whisper {self.model_size} modeli yÃ¼kleniyor...")
            self.model = whisper.load_model(self.model_size)
            print("âœ… Whisper modeli hazÄ±r")
            return True
        except ImportError:
            print("âŒ Whisper kurulu deÄŸil!")
            return False
        except Exception as e:
            print(f"âŒ Whisper yÃ¼kleme hatasÄ±: {e}")
            return False
    
    def transcribe_audio(self, audio_path, language="en"):
        """Ses dosyasÄ±nÄ± metne Ã§evir"""
        if not self.model:
            print("âŒ Whisper modeli yÃ¼klÃ¼ deÄŸil!")
            return None
        
        try:
            print(f"ğŸ¤ Ses tanÄ±ma baÅŸlÄ±yor: {audio_path}")
            
            # Whisper transcription options
            options = {
                "language": language,  # None = auto-detect, 
                "task": "transcribe",  # "transcribe" or "translate" 
                "verbose": False
            }
            
            result = self.model.transcribe(audio_path, **options)
            
            transcript_text = result["text"].strip()
            detected_language = result.get("language", "unknown")
            
            print(f"âœ… Transkripsiyon tamamlandÄ±")
            print(f"ğŸ“Š Tespit edilen dil: {detected_language}")
            print(f"ğŸ“ Metin uzunluÄŸu: {len(transcript_text)} karakter")
            
            return {
                "text": transcript_text,
                "language": detected_language,
                "segments": result.get("segments", [])
            }
            
        except Exception as e:
            print(f"âŒ Transkripsiyon hatasÄ±: {e}")
            return None

class LocalLLMAnalyzer:
    """Local LLM (Ollama) ile analiz yapan sÄ±nÄ±f"""
    
    def __init__(self, model_name="llama3.2", base_url="http://localhost:11434"):
        self.model_name = model_name
        self.base_url = base_url
        self.api_url = f"{base_url}/api/generate"
        
    def check_ollama_connection(self):
        """Ollama baÄŸlantÄ±sÄ±nÄ± kontrol et"""
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    def generate_response(self, prompt, max_tokens=1000):
        """LLM'den yanÄ±t al"""
        if not self.check_ollama_connection():
            print("âš ï¸ Ollama baÄŸlantÄ±sÄ± yok, basit analiz kullanÄ±lÄ±yor...")
            return None
            
        try:
            data = {
                "model": self.model_name,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": 0.3,
                    "top_p": 0.9,
                    "num_predict": max_tokens
                }
            }
            
            response = requests.post(self.api_url, json=data, timeout=30)
            if response.status_code == 200:
                return response.json().get('response', '').strip()
            else:
                print(f"LLM API hatasÄ±: {response.status_code}")
                return None
                
        except Exception as e:
            print(f"LLM baÄŸlantÄ± hatasÄ±: {e}")
            return None

class EnhancedTranscriptAnalyzer:
    def __init__(self, file_path, use_llm=True):
        """GeliÅŸtirilmiÅŸ transkript analiz sistemi"""
        self.file_path = file_path
        self.use_llm = use_llm
        self.text = self.load_text()
        
        # LLM analyzer'Ä± baÅŸlat
        if self.use_llm:
            self.llm = LocalLLMAnalyzer()
            if not self.llm.check_ollama_connection():
                print("ğŸ”„ Ollama bulunamadÄ±, sadece geleneksel analiz yapÄ±lacak")
                self.use_llm = False
        
        # Basit tokenization
        self.sentences = self.safe_tokenize_sentences(self.text)
        self.words = self.safe_tokenize_words(self.text.lower())
        
        # GeniÅŸletilmiÅŸ stop words listesi
        self.stop_words = {
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 
            'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'have', 'has', 'had', 
            'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might', 'must', 
            'can', 'this', 'that', 'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 
            'they', 'me', 'him', 'her', 'us', 'them', 'my', 'your', 'his', 'our', 'their',
            'so', 'very', 'just', 'now', 'also', 'only', 'here', 'there', 'where', 'when',
            'what', 'how', 'why', 'who', 'which', 'than', 'more', 'most', 'some', 'any'
        }
        
    def load_text(self):
        """Text dosyasÄ±nÄ± yÃ¼kle"""
        try:
            with open(self.file_path, 'r', encoding='utf-8') as file:
                return file.read()
        except FileNotFoundError:
            print(f"Dosya bulunamadÄ±: {self.file_path}")
            return ""
        except UnicodeDecodeError:
            # FarklÄ± encoding'leri dene
            for encoding in ['latin-1', 'cp1252', 'iso-8859-1']:
                try:
                    with open(self.file_path, 'r', encoding=encoding) as file:
                        return file.read()
                except:
                    continue
            print(f"Dosya encoding hatasÄ±: {self.file_path}")
            return ""
    
    def safe_tokenize_sentences(self, text):
        """GeliÅŸtirilmiÅŸ cÃ¼mle tokenization"""
        # Nokta, Ã¼nlem, soru iÅŸareti ile ayÄ±r
        sentences = re.split(r'[.!?]+', text)
        # BoÅŸ ve Ã§ok kÄ±sa cÃ¼mleleri filtrele
        sentences = [s.strip() for s in sentences if s.strip() and len(s.strip()) > 10]
        return sentences
    
    def safe_tokenize_words(self, text):
        """GeliÅŸtirilmiÅŸ kelime tokenization"""
        words = re.findall(r'\b\w+\b', text.lower())
        # Ã‡ok kÄ±sa kelimeleri filtrele
        words = [w for w in words if len(w) > 2]
        return words
    
    def llm_analyze_segment(self, segment_text, segment_id):
        """LLM ile segment analizi"""
        if not self.use_llm:
            return self.traditional_segment_analysis(segment_text, segment_id)
        
        prompt = f"""Bu video transkriptinin bir bÃ¶lÃ¼mÃ¼nÃ¼ analiz et:

SEGMENT {segment_id}:
"{segment_text}"

LÃ¼tfen ÅŸunlarÄ± saÄŸla:
1. ANA KONU: Bu bÃ¶lÃ¼mÃ¼n ana konusu nedir? (tek cÃ¼mle)
2. ANAHTAR MESAJLAR: En Ã¶nemli 2-3 mesaj (madde halinde)
3. DUYGUSAL TON: Pozitif/Negatif/NÃ¶tr ve neden
4. Ã–NEMLÄ° KAVRAMLAR: Bahsedilen Ã¶nemli terimler (5 tane max)
5. SEGMENT TÄ°PÄ°: Bu bÃ¶lÃ¼m nedir? (Ã¶rnek: tanÄ±m, aÃ§Ä±klama, hikaye, istatistik, tavsiye)

YanÄ±tÄ±nÄ± JSON formatÄ±nda ver:
{{
    "ana_konu": "...",
    "anahtar_mesajlar": ["...", "..."],
    "duygusal_ton": {{"ton": "pozitif/negatif/notr", "aciklama": "..."}},
    "onemli_kavramlar": ["...", "...", "..."],
    "segment_tipi": "..."
}}"""

        try:
            response = self.llm.generate_response(prompt, max_tokens=500)
            if response:
                # JSON parse etmeye Ã§alÄ±ÅŸ
                try:
                    # YanÄ±ttan JSON kÄ±smÄ±nÄ± Ã§Ä±kar
                    json_start = response.find('{')
                    json_end = response.rfind('}') + 1
                    if json_start != -1 and json_end > json_start:
                        json_str = response[json_start:json_end]
                        analysis = json.loads(json_str)
                        return analysis
                except:
                    # JSON parse edilemezse, metin analizi yap
                    return self.parse_llm_text_response(response, segment_id)
            else:
                return self.traditional_segment_analysis(segment_text, segment_id)
                
        except Exception as e:
            print(f"LLM analiz hatasÄ± (Segment {segment_id}): {e}")
            return self.traditional_segment_analysis(segment_text, segment_id)
    
    def parse_llm_text_response(self, response, segment_id):
        """LLM'nin metin yanÄ±tÄ±nÄ± parse et"""
        try:
            lines = response.split('\n')
            analysis = {
                "ana_konu": "Belirlenemedi",
                "anahtar_mesajlar": [],
                "duygusal_ton": {"ton": "nÃ¶tr", "aciklama": "LLM yanÄ±tÄ± parse edilemedi"},
                "onemli_kavramlar": [],
                "segment_tipi": "genel"
            }
            
            for line in lines:
                line = line.strip()
                if "ana konu" in line.lower() or "main topic" in line.lower():
                    analysis["ana_konu"] = line.split(':', 1)[-1].strip()
                elif "mesaj" in line.lower() or "message" in line.lower():
                    analysis["anahtar_mesajlar"].append(line.split(':', 1)[-1].strip())
                elif "ton" in line.lower() or "sentiment" in line.lower():
                    if "pozitif" in line.lower() or "positive" in line.lower():
                        analysis["duygusal_ton"]["ton"] = "pozitif"
                    elif "negatif" in line.lower() or "negative" in line.lower():
                        analysis["duygusal_ton"]["ton"] = "negatif"
            
            return analysis
            
        except:
            return self.traditional_segment_analysis("", segment_id)
    
    def traditional_segment_analysis(self, segment_text, segment_id):
        """Geleneksel segment analizi (LLM olmadan)"""
        words = self.safe_tokenize_words(segment_text)
        
        # Ana konu tahmin etme
        topic_keywords = {
            'health': ['health', 'wellness', 'medical', 'body', 'mind', 'physical', 'mental'],
            'social': ['social', 'people', 'relationship', 'community', 'friend', 'family'],
            'technology': ['technology', 'digital', 'computer', 'internet', 'data'],
            'education': ['education', 'learning', 'student', 'school', 'knowledge'],
            'business': ['business', 'work', 'career', 'job', 'company']
        }
        
        topic_scores = {}
        for topic, keywords in topic_keywords.items():
            score = sum(1 for word in words if word in keywords)
            topic_scores[topic] = score
        
        ana_konu = max(topic_scores, key=topic_scores.get) if max(topic_scores.values()) > 0 else "genel konu"
        
        # Basit sentiment analizi
        positive_words = {'good', 'great', 'excellent', 'amazing', 'wonderful', 'love', 'like', 'happy', 'joy', 'positive', 'better', 'best', 'success'}
        negative_words = {'bad', 'terrible', 'awful', 'hate', 'sad', 'angry', 'worried', 'problem', 'difficult', 'hard'}
        
        pos_count = sum(1 for word in words if word in positive_words)
        neg_count = sum(1 for word in words if word in negative_words)
        
        if pos_count > neg_count:
            ton = "pozitif"
        elif neg_count > pos_count:
            ton = "negatif"
        else:
            ton = "nÃ¶tr"
        
        # En sÄ±k geÃ§en kelimeleri bul
        word_freq = Counter([w for w in words if w not in self.stop_words])
        onemli_kavramlar = [word for word, count in word_freq.most_common(5)]
        
        return {
            "ana_konu": f"{ana_konu} konusu",
            "anahtar_mesajlar": [f"Bu bÃ¶lÃ¼mde {ana_konu} hakkÄ±nda bilgi veriliyor"],
            "duygusal_ton": {"ton": ton, "aciklama": f"{pos_count} pozitif, {neg_count} negatif kelime"},
            "onemli_kavramlar": onemli_kavramlar,
            "segment_tipi": "bilgi paylaÅŸÄ±mÄ±"
        }
    
    def llm_generate_overall_summary(self, segments_analysis):
        """LLM ile genel Ã¶zet oluÅŸtur"""
        if not self.use_llm:
            return self.traditional_overall_summary(segments_analysis)
        
        # Segment Ã¶zetlerini birleÅŸtir
        segment_summaries = []
        for i, analysis in enumerate(segments_analysis, 1):
            summary = f"Segment {i}: {analysis['ana_konu']} - {', '.join(analysis['anahtar_mesajlar'][:2])}"
            segment_summaries.append(summary)
        
        combined_segments = '\n'.join(segment_summaries)
        
        prompt = f"""Bu video transkriptinin segment analizlerine dayanarak genel bir Ã¶zet oluÅŸtur:

SEGMENT ANALÄ°ZLERÄ°:
{combined_segments}

LÃ¼tfen ÅŸunlarÄ± iÃ§eren bir Ã¶zet hazÄ±rla:
1. VÄ°DEONUN ANA KONUSU: Video genel olarak neyi anlatÄ±yor?
2. ANA MESAJLAR: Videonun vermek istediÄŸi en Ã¶nemli 3-4 mesaj
3. Ä°Ã‡ERÄ°K AKIÅI: Video nasÄ±l bir yapÄ±da ilerliyor?
4. HEDEF KÄ°TLE: Bu video kime hitap ediyor?
5. ANA Ã‡IKARIMLAR: Ä°zleyicinin Ã¶ÄŸreneceÄŸi ana noktalar

YanÄ±tÄ±nÄ± dÃ¼zenli paragraflar halinde, aÃ§Ä±k ve anlaÅŸÄ±lÄ±r ÅŸekilde yaz. 
Toplam 300-400 kelime olsun."""

        try:
            response = self.llm.generate_response(prompt, max_tokens=800)
            if response:
                return response
            else:
                return self.traditional_overall_summary(segments_analysis)
        except Exception as e:
            print(f"LLM Ã¶zet oluÅŸturma hatasÄ±: {e}")
            return self.traditional_overall_summary(segments_analysis)
    
    def traditional_overall_summary(self, segments_analysis):
        """Geleneksel Ã¶zet oluÅŸturma"""
        # En sÄ±k geÃ§en konularÄ± bul
        topics = [analysis['ana_konu'] for analysis in segments_analysis]
        topic_freq = Counter(topics)
        main_topic = topic_freq.most_common(1)[0][0] if topic_freq else "Ã§eÅŸitli konular"
        
        # Duygusal ton daÄŸÄ±lÄ±mÄ±
        tones = [analysis['duygusal_ton']['ton'] for analysis in segments_analysis]
        tone_freq = Counter(tones)
        dominant_tone = tone_freq.most_common(1)[0][0]
        
        # TÃ¼m Ã¶nemli kavramlarÄ± topla
        all_concepts = []
        for analysis in segments_analysis:
            all_concepts.extend(analysis['onemli_kavramlar'])
        
        concept_freq = Counter(all_concepts)
        top_concepts = [concept for concept, count in concept_freq.most_common(8)]
        
        summary = f"""Bu video Ã¶ncelikli olarak {main_topic} Ã¼zerine odaklanmaktadir. 
        
Video toplamda {len(segments_analysis)} anlamlÄ± bÃ¶lÃ¼mden oluÅŸuyor ve genel olarak {dominant_tone} bir ton taÅŸÄ±yor.

Video boyunca en sÄ±k bahsedilen kavramlar: {', '.join(top_concepts[:5])}.

Ä°Ã§erik, izleyicilere {main_topic} konusunda bilgi vermeyi ve farkÄ±ndalÄ±k oluÅŸturmayÄ± amaÃ§lÄ±yor. 
Video sistematik bir ÅŸekilde konuyu ele alarak, farklÄ± aÃ§Ä±lardan yaklaÅŸÄ±m sunuyor.

Bu iÃ§erik, {main_topic} ile ilgilenen herkese hitap eden, bilgilendirici bir yapÄ±ya sahip."""
        
        return summary
    
    def extract_key_insights(self):
        """Ana iÃ§gÃ¶rÃ¼leri Ã§Ä±kar"""
        insights = {
            'video_length_analysis': self.analyze_video_length(),
            'content_density': self.analyze_content_density(),
            'topic_progression': self.analyze_topic_progression(),
            'key_terminology': self.extract_key_terminology(),
            'structural_analysis': self.analyze_content_structure()
        }
        return insights
    
    def analyze_video_length(self):
        """Video uzunluÄŸu analizi"""
        word_count = len(self.words)
        sentence_count = len(self.sentences)
        
        # Ortalama konuÅŸma hÄ±zÄ± 150-160 kelime/dakika
        estimated_duration = word_count / 150  # dakika
        
        if estimated_duration < 5:
            length_category = "kÄ±sa"
        elif estimated_duration < 15:
            length_category = "orta"
        else:
            length_category = "uzun"
            
        return {
            'estimated_duration_minutes': round(estimated_duration, 1),
            'word_count': word_count,
            'sentence_count': sentence_count,
            'category': length_category,
            'density_score': round(sentence_count / max(estimated_duration, 1), 1)
        }
    
    def analyze_content_density(self):
        """Ä°Ã§erik yoÄŸunluÄŸu analizi"""
        # Benzersiz kelime oranÄ±
        unique_ratio = len(set(self.words)) / len(self.words) if self.words else 0
        
        # Ortalama cÃ¼mle uzunluÄŸu
        avg_sentence_length = len(self.words) / len(self.sentences) if self.sentences else 0
        
        # KarmaÅŸÄ±klÄ±k skoru
        complexity_score = unique_ratio * avg_sentence_length
        
        if complexity_score < 5:
            complexity_level = "basit"
        elif complexity_score < 10:
            complexity_level = "orta"
        else:
            complexity_level = "karmaÅŸÄ±k"
            
        return {
            'unique_word_ratio': round(unique_ratio, 3),
            'avg_sentence_length': round(avg_sentence_length, 1),
            'complexity_score': round(complexity_score, 2),
            'complexity_level': complexity_level
        }
    
    def analyze_topic_progression(self):
        """Konu ilerleyiÅŸi analizi"""
        segments = self.segment_content_enhanced()
        progression = []
        
        for i, segment in enumerate(segments):
            # Her segmentin temel konusunu belirle
            words = self.safe_tokenize_words(segment['text'])
            
            # Konu belirleyici kelimeler
            topic_indicators = {
                'introduction': ['introduce', 'begin', 'start', 'first', 'welcome'],
                'explanation': ['explain', 'because', 'reason', 'how', 'what', 'define'],
                'example': ['example', 'instance', 'like', 'such as', 'consider'],
                'conclusion': ['conclude', 'finally', 'end', 'summary', 'therefore'],
                'transition': ['next', 'now', 'then', 'however', 'but', 'also']
            }
            
            segment_type = 'content'  # default
            max_score = 0
            
            for topic_type, indicators in topic_indicators.items():
                score = sum(1 for word in words if word in indicators)
                if score > max_score:
                    max_score = score
                    segment_type = topic_type
            
            progression.append({
                'segment': i + 1,
                'type': segment_type,
                'word_count': len(words)
            })
        
        return progression
    
    def extract_key_terminology(self):
        """Anahtar terminoloji Ã§Ä±karÄ±mÄ±"""
        # TF-IDF ile Ã¶nemli terimleri bul
        try:
            sentences_for_tfidf = [' '.join(self.sentences[i:i+2]) for i in range(0, len(self.sentences), 2)]
            
            vectorizer = TfidfVectorizer(
                max_features=30,
                stop_words='english',
                ngram_range=(1, 3),  # 1-3 kelimelik terimler
                min_df=1
            )
            
            tfidf_matrix = vectorizer.fit_transform(sentences_for_tfidf)
            feature_names = vectorizer.get_feature_names_out()
            scores = tfidf_matrix.sum(axis=0).A1
            
            term_scores = [(feature_names[i], scores[i]) for i in range(len(feature_names))]
            term_scores.sort(key=lambda x: x[1], reverse=True)
            
            return {
                'technical_terms': [term for term, score in term_scores[:15]],
                'key_phrases': [term for term, score in term_scores if len(term.split()) > 1][:10]
            }
            
        except Exception as e:
            print(f"Terminoloji Ã§Ä±karÄ±m hatasÄ±: {e}")
            return {'technical_terms': [], 'key_phrases': []}
    
    def analyze_content_structure(self):
        """Ä°Ã§erik yapÄ±sÄ± analizi"""
        segments = self.segment_content_enhanced()
        
        structure_analysis = {
            'total_segments': len(segments),
            'segment_balance': self.calculate_segment_balance(segments),
            'content_flow': self.analyze_content_flow(segments),
            'information_distribution': self.analyze_information_distribution(segments)
        }
        
        return structure_analysis
    
    def calculate_segment_balance(self, segments):
        """Segment dengesi hesapla"""
        word_counts = [seg['word_count'] for seg in segments]
        avg_words = np.mean(word_counts)
        std_words = np.std(word_counts)
        
        balance_score = 1 - (std_words / avg_words) if avg_words > 0 else 0
        
        return {
            'average_segment_length': round(avg_words, 1),
            'length_variation': round(std_words, 1),
            'balance_score': round(balance_score, 3),
            'balance_level': 'dengeli' if balance_score > 0.7 else 'dengesiz'
        }
    
    def analyze_content_flow(self, segments):
        """Ä°Ã§erik akÄ±ÅŸÄ± analizi"""
        flow_patterns = []
        
        for i in range(len(segments) - 1):
            current_length = segments[i]['word_count']
            next_length = segments[i + 1]['word_count']
            
            if next_length > current_length * 1.5:
                flow_patterns.append('expansion')
            elif next_length < current_length * 0.5:
                flow_patterns.append('contraction')
            else:
                flow_patterns.append('stable')
        
        pattern_freq = Counter(flow_patterns)
        
        return {
            'flow_patterns': dict(pattern_freq),
            'dominant_pattern': pattern_freq.most_common(1)[0][0] if pattern_freq else 'stable'
        }
    
    def analyze_information_distribution(self, segments):
        """Bilgi daÄŸÄ±lÄ±mÄ± analizi"""
        total_words = sum(seg['word_count'] for seg in segments)
        
        distribution = []
        for i, seg in enumerate(segments):
            percentage = (seg['word_count'] / total_words) * 100
            distribution.append({
                'segment': i + 1,
                'percentage': round(percentage, 1),
                'word_count': seg['word_count']
            })
        
        # En bilgi yoÄŸun segmentleri bul
        sorted_segments = sorted(distribution, key=lambda x: x['percentage'], reverse=True)
        
        return {
            'distribution': distribution,
            'most_dense_segments': sorted_segments[:3],
            'information_concentration': 'concentrated' if sorted_segments[0]['percentage'] > 30 else 'distributed'
        }
    
    def segment_content_enhanced(self):
        """GeliÅŸtirilmiÅŸ iÃ§erik segmentasyonu"""
        segments = []
        current_segment = []
        word_count = 0
        
        for sentence in self.sentences:
            sentence_words = len(sentence.split())
            current_segment.append(sentence)
            word_count += sentence_words
            
            # Segment sonu koÅŸullarÄ± (daha akÄ±llÄ±)
            should_break = False
            
            # Uzunluk bazlÄ± koÅŸullar
            if word_count > 200:  # Minimum segment uzunluÄŸu
                # DoÄŸal kÄ±rÄ±lma noktalarÄ±nÄ± ara
                if any(phrase in sentence.lower() for phrase in [
                    'next', 'now', 'then', 'however', 'but', 'also', 'another',
                    'furthermore', 'moreover', 'additionally', 'in conclusion'
                ]):
                    should_break = True
                elif word_count > 300:  # Maksimum segment uzunluÄŸu
                    should_break = True
            
            # Son cÃ¼mle ise kesinlikle kÄ±r
            if sentence == self.sentences[-1]:
                should_break = True
            
            if should_break and current_segment:
                segment_text = ' '.join(current_segment)
                segments.append({
                    'segment_id': len(segments) + 1,
                    'text': segment_text,
                    'sentence_count': len(current_segment),
                    'word_count': word_count,
                    'char_count': len(segment_text)
                })
                current_segment = []
                word_count = 0
        
        return segments
    
    def save_enhanced_results(self, results):
        """GeliÅŸtirilmiÅŸ sonuÃ§larÄ± kaydet"""
        # Dizinleri oluÅŸtur
        os.makedirs("output/enhanced_analysis", exist_ok=True)
        os.makedirs("output/llm_insights", exist_ok=True)
        os.makedirs("output/structured_data", exist_ok=True)
        
        # 1. LLM Ä°Ã§gÃ¶rÃ¼leri
        with open("output/llm_insights/segment_analysis.txt", "w", encoding="utf-8") as f:
            f.write("LLM DESTEKLÄ° SEGMENT ANALÄ°ZÄ°\n")
            f.write("=" * 50 + "\n\n")
            
            for i, analysis in enumerate(results['llm_segment_analysis'], 1):
                f.write(f"SEGMENT {i}\n")
                f.write("-" * 20 + "\n")
                f.write(f"Ana Konu: {analysis['ana_konu']}\n")
                f.write(f"Segment Tipi: {analysis['segment_tipi']}\n")
                f.write(f"Duygusal Ton: {analysis['duygusal_ton']['ton']} ({analysis['duygusal_ton']['aciklama']})\n")
                f.write(f"\nAnahtar Mesajlar:\n")
                for mesaj in analysis['anahtar_mesajlar']:
                    f.write(f"â€¢ {mesaj}\n")
                f.write(f"\nÃ–nemli Kavramlar: {', '.join(analysis['onemli_kavramlar'])}\n")
                f.write("\n" + "="*50 + "\n\n")
        
        # 2. Genel Ã–zet
        with open("output/llm_insights/overall_summary.txt", "w", encoding="utf-8") as f:
            f.write("VIDEO GENEL Ã–ZETÄ° (LLM Destekli)\n")
            f.write("=" * 50 + "\n\n")
            f.write(results['overall_summary'])
            f.write("\n\n")
        
        # 3. Anahtar Ä°Ã§gÃ¶rÃ¼ler
        with open("output/enhanced_analysis/key_insights.txt", "w", encoding="utf-8") as f:
            f.write("ANAHTAR Ä°Ã‡GÃ–RÃœLER VE DERINLEMESINE ANALÄ°Z\n")
            f.write("=" * 60 + "\n\n")
            
            insights = results['key_insights']
            
            # Video uzunluk analizi
            length_analysis = insights['video_length_analysis']
            f.write("1. VÄ°DEO UZUNLUK ANALÄ°ZÄ°\n")
            f.write("-" * 30 + "\n")
            f.write(f"Tahmini SÃ¼re: {length_analysis['estimated_duration_minutes']} dakika\n")
            f.write(f"Toplam Kelime: {length_analysis['word_count']}\n")
            f.write(f"Toplam CÃ¼mle: {length_analysis['sentence_count']}\n")
            f.write(f"Video Kategorisi: {length_analysis['category']}\n")
            f.write(f"Ä°Ã§erik YoÄŸunluk Skoru: {length_analysis['density_score']}\n\n")
            
            # Ä°Ã§erik yoÄŸunluÄŸu
            density = insights['content_density']
            f.write("2. Ä°Ã‡ERÄ°K YOÄUNLUK ANALÄ°ZÄ°\n")
            f.write("-" * 30 + "\n")
            f.write(f"Benzersiz Kelime OranÄ±: {density['unique_word_ratio']}\n")
            f.write(f"Ortalama CÃ¼mle UzunluÄŸu: {density['avg_sentence_length']} kelime\n")
            f.write(f"KarmaÅŸÄ±klÄ±k Seviyesi: {density['complexity_level']}\n")
            f.write(f"KarmaÅŸÄ±klÄ±k Skoru: {density['complexity_score']}\n\n")
            
            # Konu ilerleyiÅŸi
            progression = insights['topic_progression']
            f.write("3. KONU Ä°LERLEYÄ°ÅÄ° ANALÄ°ZÄ°\n")
            f.write("-" * 30 + "\n")
            for item in progression:
                f.write(f"Segment {item['segment']}: {item['type']} ({item['word_count']} kelime)\n")
            f.write("\n")
            
            # Anahtar terminoloji
            terminology = insights['key_terminology']
            f.write("4. ANAHTAR TERMÄ°NOLOJÄ°\n")
            f.write("-" * 30 + "\n")
            f.write(f"Teknik Terimler: {', '.join(terminology['technical_terms'][:10])}\n")
            f.write(f"Ã–nemli Ä°fadeler: {', '.join(terminology['key_phrases'][:5])}\n\n")
            
            # YapÄ±sal analiz
            structure = insights['structural_analysis']
            f.write("5. YAPISAL ANALÄ°Z\n")
            f.write("-" * 30 + "\n")
            f.write(f"Toplam Segment: {structure['total_segments']}\n")
            f.write(f"Ortalama Segment UzunluÄŸu: {structure['segment_balance']['average_segment_length']}\n")
            f.write(f"Ä°Ã§erik Dengesi: {structure['segment_balance']['balance_level']}\n")
            f.write(f"Bilgi DaÄŸÄ±lÄ±mÄ±: {structure['information_distribution']['information_concentration']}\n")
            f.write(f"Dominant AkÄ±ÅŸ Paterni: {structure['content_flow']['dominant_pattern']}\n\n")
        
        # 4. YapÄ±landÄ±rÄ±lmÄ±ÅŸ veri (JSON)
        with open("output/structured_data/analysis_data.json", "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        # 5. Ã–zet rapor
        with open("output/enhanced_analysis/executive_summary.txt", "w", encoding="utf-8") as f:
            f.write("YÃ–NETÄ°CÄ° Ã–ZETÄ° - VIDEO ANALÄ°Z RAPORU\n")
            f.write("=" * 50 + "\n\n")
            
            length_info = results['key_insights']['video_length_analysis']
            f.write(f"ğŸ“Š VIDEO Ä°STATÄ°STÄ°KLERÄ°:\n")
            f.write(f"â€¢ Tahmini SÃ¼re: {length_info['estimated_duration_minutes']} dakika\n")
            f.write(f"â€¢ Toplam Kelime: {length_info['word_count']:,}\n")
            f.write(f"â€¢ Video Kategorisi: {length_info['category']}\n\n")
            
            f.write(f"ğŸ¯ ANA BULGULAR:\n")
            f.write(f"â€¢ Toplam {len(results['llm_segment_analysis'])} anlamlÄ± segment tespit edildi\n")
            
            # En sÄ±k geÃ§en konularÄ± listele
            topics = [seg['ana_konu'] for seg in results['llm_segment_analysis']]
            topic_freq = Counter(topics)
            f.write(f"â€¢ En yaygÄ±n konu kategorileri: {', '.join([topic for topic, count in topic_freq.most_common(3)])}\n")
            
            # Duygusal ton daÄŸÄ±lÄ±mÄ±
            tones = [seg['duygusal_ton']['ton'] for seg in results['llm_segment_analysis']]
            tone_freq = Counter(tones)
            f.write(f"â€¢ Duygusal ton daÄŸÄ±lÄ±mÄ±: {dict(tone_freq)}\n\n")
            
            f.write(f"ğŸ“ˆ Ä°Ã‡ERÄ°K KALÄ°TESÄ°:\n")
            density_info = results['key_insights']['content_density']
            f.write(f"â€¢ KarmaÅŸÄ±klÄ±k Seviyesi: {density_info['complexity_level']}\n")
            f.write(f"â€¢ Ä°Ã§erik YoÄŸunluÄŸu: {density_info['complexity_score']}/10\n\n")
            
            f.write(f"ğŸ” Ã–NERÄ°LER:\n")
            if density_info['complexity_level'] == 'karmaÅŸÄ±k':
                f.write("â€¢ Ä°Ã§erik karmaÅŸÄ±k seviyede, hedef kitleye uygunluÄŸu deÄŸerlendirilmeli\n")
            if length_info['category'] == 'uzun':
                f.write("â€¢ Video uzun kategorisinde, bÃ¶lÃ¼mlere ayrÄ±lmasÄ± dÃ¼ÅŸÃ¼nÃ¼lebilir\n")
            
            structure_info = results['key_insights']['structural_analysis']
            if structure_info['segment_balance']['balance_level'] == 'dengesiz':
                f.write("â€¢ Segment uzunluklarÄ± dengesiz, iÃ§erik yapÄ±sÄ± gÃ¶zden geÃ§irilmeli\n")
            
            f.write(f"\nğŸ“„ DETAYLAR:\n")
            f.write(f"â€¢ Tam analiz sonuÃ§larÄ±: output/llm_insights/ klasÃ¶rÃ¼nde\n")
            f.write(f"â€¢ YapÄ±landÄ±rÄ±lmÄ±ÅŸ veriler: output/structured_data/ klasÃ¶rÃ¼nde\n")
            f.write(f"â€¢ Rapor oluÅŸturulma zamanÄ±: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
        
        print("\nâœ… SONUÃ‡LAR KAYDEDÄ°LDÄ°:")
        print("ğŸ“ output/llm_insights/segment_analysis.txt - DetaylÄ± segment analizleri")
        print("ğŸ“ output/llm_insights/overall_summary.txt - Genel video Ã¶zeti") 
        print("ğŸ“ output/enhanced_analysis/key_insights.txt - Derinlemesine iÃ§gÃ¶rÃ¼ler")
        print("ğŸ“ output/enhanced_analysis/executive_summary.txt - YÃ¶netici Ã¶zeti")
        print("ğŸ“ output/structured_data/analysis_data.json - YapÄ±landÄ±rÄ±lmÄ±ÅŸ veriler")
    
    def run_complete_analysis(self):
        """Tam analizi Ã§alÄ±ÅŸtÄ±r"""
        print("ğŸš€ GeliÅŸtirilmiÅŸ Video Analiz Sistemi BaÅŸlatÄ±lÄ±yor...")
        print(f"ğŸ“„ Analiz edilen dosya: {self.file_path}")
        print(f"ğŸ¤– LLM DesteÄŸi: {'Aktif' if self.use_llm else 'Pasif'}")
        print("-" * 60)
        
        if not self.text:
            print("âŒ Dosya okunamadÄ± veya boÅŸ!")
            return None
        
        # 1. Ä°Ã§erik segmentasyonu
        print("ğŸ“Š 1. Ä°Ã§erik segmentasyonu yapÄ±lÄ±yor...")
        segments = self.segment_content_enhanced()
        
        if not segments:
            print("âŒ Segment oluÅŸturulamadÄ±!")
            return None
        
        print(f"âœ… {len(segments)} segment oluÅŸturuldu")
        
        # 2. LLM ile segment analizi
        print("ğŸ§  2. Segment analizi yapÄ±lÄ±yor...")
        llm_analyses = []
        
        for i, segment in enumerate(segments, 1):
            print(f"   ğŸ“ Segment {i}/{len(segments)} analiz ediliyor...")
            analysis = self.llm_analyze_segment(segment['text'], i)
            llm_analyses.append(analysis)
        
        print("âœ… Segment analizleri tamamlandÄ±")
        
        # 3. Genel Ã¶zet oluÅŸturma
        print("ğŸ“‹ 3. Genel Ã¶zet oluÅŸturuluyor...")
        overall_summary = self.llm_generate_overall_summary(llm_analyses)
        print("âœ… Genel Ã¶zet hazÄ±rlandÄ±")
        
        # 4. Anahtar iÃ§gÃ¶rÃ¼ler
        print("ğŸ” 4. Anahtar iÃ§gÃ¶rÃ¼ler Ã§Ä±karÄ±lÄ±yor...")
        key_insights = self.extract_key_insights()
        print("âœ… Ä°Ã§gÃ¶rÃ¼ler analiz edildi")
        
        # 5. SonuÃ§larÄ± birleÅŸtir
        results = {
            'analysis_metadata': {
                'file_path': self.file_path,
                'llm_enabled': self.use_llm,
                'analysis_timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
                'total_segments': len(segments)
            },
            'segments': segments,
            'llm_segment_analysis': llm_analyses,
            'overall_summary': overall_summary,
            'key_insights': key_insights
        }
        
        # 6. SonuÃ§larÄ± kaydet
        print("ğŸ’¾ 5. SonuÃ§lar kaydediliyor...")
        self.save_enhanced_results(results)
        
        print("\nğŸ‰ ANALÄ°Z TAMAMLANDI!")
        return results

def complete_video_analysis_pipeline(video_path, use_llm=True, whisper_model="base", language=None):
    """Tam video analiz pipeline'Ä±"""
    
    # 1. BaÄŸÄ±mlÄ±lÄ±k kontrolÃ¼
    video_processor = VideoProcessor()
    deps = video_processor.check_dependencies()
    
    if not deps['ffmpeg']:
        print("âŒ FFmpeg bulunamadÄ±! Video iÅŸleme yapÄ±lamaz.")
        print("Kurulum:")
        print("  Windows: https://ffmpeg.org/download.html")
        print("  macOS: brew install ffmpeg")
        print("  Linux: sudo apt install ffmpeg")
        return None
    
    if not deps['whisper']:
        print("âŒ Whisper bulunamadÄ±! Ses tanÄ±ma yapÄ±lamaz.")
        print("YÃ¼klemek iÃ§in: pip install openai-whisper")
        return None
    
    # 2. Video'dan ses Ã§Ä±kar
    audio_path = video_processor.extract_audio_from_video(video_path)
    if not audio_path:
        return None
    
    # 3. Ses tanÄ±ma ile transkript oluÅŸtur
    transcriber = WhisperTranscriber(model_size=whisper_model)
    transcript_result = transcriber.transcribe_audio(audio_path, language=language)
    
    if not transcript_result:
        return None
    
    # 4. Transkripti dosyaya kaydet
    transcript_text = transcript_result["text"]
    video_name = Path(video_path).stem
    transcript_path = f"output/{video_name}_transcript.txt"
    
    os.makedirs("output", exist_ok=True)
    with open(transcript_path, 'w', encoding='utf-8') as f:
        f.write(transcript_text)
    
    print(f"ğŸ“„ Transkript kaydedildi: {transcript_path}")
    
    # 5. Ana analiz sistemini Ã§alÄ±ÅŸtÄ±r
    analyzer = EnhancedTranscriptAnalyzer(transcript_path, use_llm=use_llm)
    results = analyzer.run_complete_analysis()
    
    # 6. GeÃ§ici ses dosyasÄ±nÄ± temizle
    try:
        os.remove(audio_path)
        print(f"ğŸ—‘ï¸ GeÃ§ici dosya temizlendi: {audio_path}")
    except:
        pass
    
    return results

def main():
    """Ana Ã§alÄ±ÅŸtÄ±rma fonksiyonu - Video Processing Dahil"""
    print("ğŸ¬ TAM VÄ°DEO ANALÄ°Z SÄ°STEMÄ°")
    print("Video'dan Metne + LLM Destekli Analiz")
    print("=" * 60)
    
    # Dosya tipini belirle
    input_type = input("""
ğŸ“ Girdi tipi seÃ§in:
1. Video dosyasÄ± (.mp4, .avi, .mov, .mkv vb.) ğŸ¥
2. HazÄ±r transkript dosyasÄ± (.txt) ğŸ“„
SeÃ§iminiz (1/2): """).strip()
    
    if input_type == "1":
        # Video dosyasÄ± analizi
        video_path = input("ğŸ¥ Video dosya yolunu girin: ").strip()
        
        if not os.path.exists(video_path):
            print(f"âŒ Video dosyasÄ± bulunamadÄ±: {video_path}")
            return
        
        # Video analiz parametreleri
        print("\nğŸ”§ AYARLAR:")
        whisper_model = input("ğŸ¤– Whisper model boyutu (tiny/base/small/medium/large) [base]: ").strip() or "base"
        
        language_input = input("ğŸŒ Dil belirtin (en/auto) [auto]: ").strip() or "auto"
        language = None if language_input == "auto" else language_input
        
        use_llm = input("ğŸ§  LLM desteÄŸi kullanÄ±lsÄ±n mÄ±? (y/n) [y]: ").strip().lower() != 'n'
        
        print(f"\nğŸš€ TAM VÄ°DEO ANALÄ°Z PIPELINE'I BAÅLATILIYOR...")
        print(f"ğŸ“¹ Video: {Path(video_path).name}")
        print(f"ğŸ¤– Whisper Model: {whisper_model}")
        print(f"ğŸŒ Dil: {language or 'otomatik tespit'}")
        print(f"ğŸ§  LLM: {'Aktif' if use_llm else 'Pasif'}")
        print("-" * 60)
        
        results = complete_video_analysis_pipeline(video_path, use_llm, whisper_model, language)
        
    elif input_type == "2":
        # HazÄ±r transkript analizi
        transcript_path = input("ğŸ“„ Transkript dosya yolunu girin: ").strip()
        
        if not os.path.exists(transcript_path):
            print(f"âŒ Transkript dosyasÄ± bulunamadÄ±: {transcript_path}")
            return
        
        use_llm = input("ğŸ§  LLM desteÄŸi kullanÄ±lsÄ±n mÄ±? (y/n) [y]: ").strip().lower() != 'n'
        
        print(f"\nğŸš€ TRANSKRÄ°PT ANALÄ°Z SÄ°STEMÄ° BAÅLATILIYOR...")
        print(f"ğŸ“„ Dosya: {Path(transcript_path).name}")
        print(f"ğŸ§  LLM: {'Aktif' if use_llm else 'Pasif'}")
        print("-" * 60)
        
        # Analiz sistemini Ã§alÄ±ÅŸtÄ±r
        analyzer = EnhancedTranscriptAnalyzer(transcript_path, use_llm=use_llm)
        results = analyzer.run_complete_analysis()
    
    else:
        print("âŒ GeÃ§ersiz seÃ§im!")
        return
    
    if results:
        print(f"\nğŸ“Š ANALÄ°Z Ä°STATÄ°STÄ°KLERÄ°:")
        print(f"â€¢ Toplam kelime: {results['key_insights']['video_length_analysis']['word_count']:,}")
        print(f"â€¢ Toplam cÃ¼mle: {results['key_insights']['video_length_analysis']['sentence_count']:,}")
        print(f"â€¢ Segment sayÄ±sÄ±: {len(results['segments'])}")
        print(f"â€¢ Tahmini video sÃ¼resi: {results['key_insights']['video_length_analysis']['estimated_duration_minutes']} dakika")
        print(f"â€¢ KarmaÅŸÄ±klÄ±k seviyesi: {results['key_insights']['content_density']['complexity_level']}")
        
        print("\nğŸ“ Ã‡IKTI DOSYALARI:")
        print("â€¢ output/llm_insights/segment_analysis.txt")
        print("â€¢ output/llm_insights/overall_summary.txt") 
        print("â€¢ output/enhanced_analysis/key_insights.txt")
        print("â€¢ output/enhanced_analysis/executive_summary.txt")
        print("â€¢ output/structured_data/analysis_data.json")
        print("â€¢ output/structured_data/segments_data.csv")
        
        if input_type == "1":
            video_name = Path(video_path).stem
            print(f"â€¢ output/{video_name}_transcript.txt")
        
        print(f"\nğŸ‰ ANALÄ°Z BAÅARIYLA TAMAMLANDI!")
        print("ğŸ“‚ TÃ¼m sonuÃ§lar 'output/' klasÃ¶rÃ¼nde kaydedildi.")
    else:
        print("\nâŒ Analiz baÅŸarÄ±sÄ±z oldu!")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\nâš ï¸ Ä°ÅŸlem kullanÄ±cÄ± tarafÄ±ndan durduruldu.")
    except Exception as e:
        print(f"\nâŒ Beklenmeyen hata: {e}")
        import traceback
        traceback.print_exc()